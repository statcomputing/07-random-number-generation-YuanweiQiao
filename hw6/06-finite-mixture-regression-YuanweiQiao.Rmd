---
title: "06-finite-mixture-regression"
author: "Yuanwei Qiao"
date: "10/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 4.8.1

#1 Verify the validity of the provided E- and M- steps

The complete log-likelihood function for the mixture finite regression model is given by
$$
\log l(\theta)=\sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
$$
E-step: we construct the conditional expectation over the latent variables $Z_{1}, \ldots, Z_{n}$ with
respect to the $k$ th iterate $\Psi^{(m)}$ to get the conjugate function

$$
\begin{aligned}
Q\left(\Psi ; \Psi^{(k)}\right) &=\mathbb{E}_{\theta^{(k)}}\left[\log l(\Psi) \mid Y_{1}=y_{1}, \ldots, Y_{n}=y_{n}\right] \\
&=\sum_{j=1}^{m} \sum_{i=1}^{n} \mathbb{E}_{\Psi^{(k)}}\left[z_{i j} \mid Y_{1}=y_{1}, \ldots, Y_{n}=y_{n}\right] \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
\end{aligned}
$$
where
$$
\mathbf{P}_{\psi}\left(Z_{i j}=1 \mid Y_{i}=y_{i}\right)=P_{i j}^{(k)}=\frac{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)}{\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)}
$$
thus
$$
Q\left(\Psi ; \Psi^{(k)}\right)=\sum_{j=1}^{m} \sum_{i=1}^{n} P_{i j}^{(k)} \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
$$
M-step: Using the conjugate function $Q\left(\Psi ; \Psi^{(k)}\right),$ the M-step can be conducted by solving
the first-order condition of the following equation:
$$
\frac{\partial Q\left(\Psi ; \Psi^{(k)}\right)}{\partial \Psi}=0
$$
which yields the ML estimate of the parameters.
For the parameter: $\pi_{j}$ where $\mathrm{j}=1,2, \ldots, \mathrm{m}$
$$
\frac{\partial Q\left(\Psi ; \Psi^{(k)}\right)}{\partial \pi_{\mathrm{j}}}=0 \Rightarrow \pi_{i j}^{(k+1)}=\frac{\sum_{i=1}^{n} P_{i j}^{(k+1)}}{n}
$$
The same as to $\beta_{j}$ and $\sigma$

#2 Implement of EM algorithm in R


The r code of EM algorithm to estimate the parameters of the finite mixture regression
model are:

```{r code, results='hide'}
mymcmc <- function(niter, thetaInit, data, sigma2, tau2, nburn= 100) {
  p <- length(thetaInit)
  thetaCurrent <- thetaInit
  ## define a function for full conditional sampling  
  logFC <- function(th, idx) {
    theta <- thetaCurrent
    theta[idx] <- th
    logpost(theta, data, sigma2, tau2)
  }
  out <- matrix(thetaInit, niter, p, byrow = TRUE)
  print(out)
  ## Gibbs sampling
  for (i in 2:niter) {
    for (j in 1:p) {
      ## general-purpose arms algorithm
      out[i, j] <- thetaCurrent[j] <-
          HI::arms(thetaCurrent[j], logFC,
                   function(x, idx) ((x > -10) * (x < 10)), 
                   1, idx = j)
    }
  }
  out[-(1:nburn), ]
}


niter <- 600
nburn <- 100
n <- 100
a <- 0.0; b <- 0.5
x <- rnorm(n)
y <- rpois(n, exp(a + b * x))
mydata <- data.frame(y = y, x = x)

thetaInit <- c(2, 2, 2, 2, 2)
print(thetaInit)
sigma2 <- tau2 <- 100
sim <- mymcmc(niter, thetaInit, mydata, sigma2, tau2)
#print(sim)

plot(ts(sim[,1]))
plot(ts(sim[,2]))
plot(ts(sim[,3]))
plot(ts(sim[,4]))
plot(ts(sim[,5]))
```









