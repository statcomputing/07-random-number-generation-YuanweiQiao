---
title: "06-finite-mixture-regression"
author: "Yuanwei Qiao"
date: "10/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 4.8.1

#1 Verify the validity of the provided E- and M- steps

The complete log-likelihood function for the mixture finite regression model is given by
$$
\log l(\theta)=\sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
$$
E-step: we construct the conditional expectation over the latent variables $Z_{1}, \ldots, Z_{n}$ with
respect to the $k$ th iterate $\Psi^{(m)}$ to get the conjugate function

$$
\begin{aligned}
Q\left(\Psi ; \Psi^{(k)}\right) &=\mathbb{E}_{\theta^{(k)}}\left[\log l(\Psi) \mid Y_{1}=y_{1}, \ldots, Y_{n}=y_{n}\right] \\
&=\sum_{j=1}^{m} \sum_{i=1}^{n} \mathbb{E}_{\Psi^{(k)}}\left[z_{i j} \mid Y_{1}=y_{1}, \ldots, Y_{n}=y_{n}\right] \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
\end{aligned}
$$
where
$$
\mathbf{P}_{\psi}\left(Z_{i j}=1 \mid Y_{i}=y_{i}\right)=P_{i j}^{(k)}=\frac{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)}{\sum_{j=1}^{m} \pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)}
$$
thus
$$
Q\left(\Psi ; \Psi^{(k)}\right)=\sum_{j=1}^{m} \sum_{i=1}^{n} P_{i j}^{(k)} \log \left\{\pi_{j} \phi\left(y_{i}-x_{i}^{T} \beta_{j} ; 0, \sigma^{2}\right)\right\}
$$
M-step: Using the conjugate function $Q\left(\Psi ; \Psi^{(k)}\right),$ the M-step can be conducted by solving
the first-order condition of the following equation:
$$
\frac{\partial Q\left(\Psi ; \Psi^{(k)}\right)}{\partial \Psi}=0
$$
which yields the ML estimate of the parameters.
For the parameter: $\pi_{j}$ where $\mathrm{j}=1,2, \ldots, \mathrm{m}$
$$
\frac{\partial Q\left(\Psi ; \Psi^{(k)}\right)}{\partial \pi_{\mathrm{j}}}=0 \Rightarrow \pi_{i j}^{(k+1)}=\frac{\sum_{i=1}^{n} P_{i j}^{(k+1)}}{n}
$$
The same as to $\beta_{j}$ and $\sigma$

#2 Implement of EM algorithm in R


The r code of EM algorithm to estimate the parameters of the finite mixture regression
model are:

```{r code, results='hide'}
# EM algorithm manually
# Needed functions
mysum <- function(x) {
 sum(x[is.finite(x)])
}
LogLike <- function(tau,pi.init,beta.init,sigma.init){
 compJhat <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
  compJhat[,j] <- tau[,j]*(log(pi.init[j])+log(dnorm(y-xmat%*%beta.init[,j], 0,
sigma.init)))
 }
 LogLike <- mysum(rowSums(compJhat))
}

SumLike <- function(pi.init,beta.init,sigma.init){
 hat <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
  hat[,j] <- pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init)
 }
 SumLike <- rowSums(hat)
}

# Function of finite mixture regression
regmix_em <- function(y,xmat,pi.init,beta.init,sigma.init,tol,maxit){
 tau <- array(NA,dim = c(n,length(pi.init)))
 loglik <- c()
 loglik[1] <- 0
 k <- 2
 iter <- 1

 # compute initial log-likelihood 1&2
 compJ <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
  compJ[,j] <- pi.init[j]*(log(pi.init[j])+log(dnorm(y-xmat%*%beta.init[,j],0,sigma.init)))
 }

 loglik[2] <- mysum(rowSums(compJ))

 # loop
 while(abs(loglik[k]-loglik[k-1]) >= tol && iter < maxit){
  # E step
  for(j in 1:length(pi.init)){
    dem <- SumLike(pi.init,beta.init,sigma.init)
    tau[,j] <- (pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))/dem
  }

 # M step
 for(j in 1:length(pi.init)){
  pi.init[j] <- mysum(tau[,j])/n
 }

 for(j in 1:length(pi.init)){
  beta.init[,j] <- as.matrix(rowSums(t(xmat)%*%(tau[,j] %*% t(y)))) / mysum((xmat
%*% t(xmat)) %*% tau[,j])
 }

 sigmaJ <- c()
 for(j in 1:length(pi.init)){
  sigmaJ[j] <- mysum(tau[,j]*(y-xmat%*%beta.init[,j])^2)
 }
 sigma.init <- sqrt(mysum(sigmaJ)/n)

 loglik[k+1] <- LogLike(tau,pi.init,beta.init,sigma.init)

 iter <- iter+1
 k <- k+1
}

regmix_em <- list(k = k,
          pi.hat = pi.init,
          sigma.hat = sigma.init,
          beta.hat = beta.init,
          log.lik = loglik[k])
}




#3 Generating data and estimating the parameters

#function to generate data from mixture regression model
regmix_sim <- function(n, pi, beta, sigma) {
    K <- ncol(beta)
    p <- NROW(beta)
    xmat <- matrix(rnorm(n * p), n, p) # normal covaraites
    error <- matrix(rnorm(n * K, sd = sigma), n, K)
    ymat <- xmat %*% beta + error # n by K matrix
    ind <- t(rmultinom(n, size = 1, prob = pi))
    y <- rowSums(ymat * ind)
    data.frame(y, xmat)
}

##Parameters
n <- 400
pi <- c(.3, .4, .3)
bet <- matrix(c( 1,  1,  1, -1, -1, -1), 2, 3)
sig <- 1
set.seed(1205)


#generating data
dat <- regmix_sim(n, pi, bet, sig)
dat <- as.matrix(dat)

dat[1:10,] #print the generated data from row 1 to row 10


#Estimating Parameters using EM algorthim
y <- dat[,1]
xmat <- dat[,-1]
pi.init <- pi/pi/length(pi)
sigma.init <- sig/sig
beta.init <- matrix(rnorm(6), 2, 3)
tol <- 1e-5
maxit <- 500

MLE_EM <-  regmix_em(y,xmat,pi.init,beta.init,sigma.init,tol,maxit)

#Print Results
MLE_EM$k #number of iterations
MLE_EM$pi.hat 
MLE_EM$beta.hat 
MLE_EM$sigma.hat 
MLE_EM$log.lik 





# 4.8.2

#1 Write a function regmix_em1step() to implement the one step EM iteration:


regmix_em1step <- function(y,xmat,pi.init,beta.init,sigma.init){
 tau <- array(NA,dim = c(length(y),length(pi.init)))
 # E step
 for(j in 1:length(pi.init)){
  dem <- SumLike(pi.init,beta.init,sigma.init)
  tau[,j] <- (pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))/dem
 }

 # M step
 for(j in 1:length(pi.init)){
  pi.init[j] <- mysum(tau[,j])/length(y)
 }

 for(j in 1:length(pi.init)){
  beta.init[,j] <- as.matrix(rowSums(t(xmat) %*% (tau[,j] %*% t(y)))) / mysum((xmat %*% t(xmat))%*%tau[,j])
 }

 sigmaJ <- c()
 for(j in 1:length(pi.init)){
  sigmaJ[j] <- mysum(tau[,j]*(y-xmat%*%beta.init[,j])^2)
 }
 sigma.init <- sqrt(mysum(sigmaJ)/n)

 regmix_em1step <- list(pi.hat = pi.init,
              sigma.hat = sigma.init,
              beta.hat = beta.init)
}



#2 implementation of the EM algorithm

MLE_EM1step <- regmix_em1step(y,xmat,pi.init,beta.init,sigma.init)

#print results
MLE_EM1step$pi.hat
MLE_EM1step$beta.hat
MLE_EM1step$beta.hat

#3 Call SQUAREM package

# Objective function
# negative log-likelihood of finite mixture regression
regmix_em.loglik <- function(pi.init,beta.init,sigma.init){
 compJ <- array(NA,dim = c(length(y),length(pi.init)))
 for(j in 1:3){
 compJ[,j] <- log(pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))
 }
 loglik <- -mysum(rowSums(compJ))
 return(loglik)
}
## call SQUARM package
library(SQUAREM)
fit <- squarem(par=list(pi.init=MLE_EM$pi.hat,beta.init=MLE_EM$beta.hat,sigma.init=MLE_EM$sigma.hat), objects = regmix_em.loglik,fixptfn=regmix_em1step,pi.init=MLE_EM$pi.hat, beta.init=MLE_EM$beta.hat,sigma.init=MLE_EM$sigma.hat)


remove(list=objects())
options(warn = -1)

# Generating data from finite mixture regression model
regmix_sim <- function(n,pi,beta,sigma){
 k <- ncol(beta)
 p <- nrow(beta)
 #Normal covaraites
 xmat <- matrix(rnorm(n * p), n, p)
 #Error term
 error <- matrix(rnorm(n * k, sd = sigma), n, k)
 # n by k matrix
 ymat <- xmat %*% beta + error
 ind <- t(rmultinom(n, size = 1, prob = pi))
 y <- rowSums(ymat * ind)
 data.frame(y,xmat)
}
## Estimation function (MLE and EM algorthim)
# EM algorithm manually
# Needed functions
mysum <- function(x) {
 sum(x[is.finite(x)])
}
LogLike <- function(tau,pi.init,beta.init,sigma.init){
 compJhat <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
 compJhat[,j] <- tau[,j]*(log(pi.init[j])+log(dnorm(y-xmat %*% beta.init[,j], 0, sigma.init)))
 }
 LogLike <- mysum(rowSums(compJhat))
}
SumLike <- function(pi.init,beta.init,sigma.init){
 hat <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
 hat[,j] <- pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init)
 }
 SumLike <- rowSums(hat)
}
## Function of finite mixture regression
regmix_em <- function(y,xmat,pi.init,beta.init,sigma.init,tol,maxit){
 tau <- array(NA,dim = c(n,length(pi.init)))
 loglik <- c()
 loglik[1] <- 0
 k <- 2
 iter <- 1

 ## compute initial loglikelihood 1&2
 compJ <- array(NA,dim = c(n,length(pi.init)))
 for(j in 1:length(pi.init)){
 compJ[,j] <- pi.init[j]*(log(pi.init[j])+log(dnorm(y-xmat%*%beta.init[,j],0,sigma.init)))
 }
 loglik[2] <- mysum(rowSums(compJ))

 # loop
 while(abs(loglik[k]-loglik[k-1]) >= tol && iter < maxit){
 # E step
 for(j in 1:length(pi.init)){
 dem <- SumLike(pi.init,beta.init,sigma.init)
 tau[,j] <- (pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))/dem
 }

 # M step
 for(j in 1:length(pi.init)){
 pi.init[j] <- mysum(tau[,j])/n
 }

 for(j in 1:length(pi.init)){
 beta.init[,j] <- as.matrix(rowSums(t(xmat) %*% (tau[,j] %*% t(y)))) / mysum((xmat %*% t(xmat)) %*% tau[,j])
 }
 sigmaJ <- c()
 for(j in 1:length(pi.init)){
 sigmaJ[j] <- mysum(tau[,j]*(y-xmat%*%beta.init[,j])^2)
 }
 sigma.init <- sqrt(mysum(sigmaJ)/n)

 loglik[k+1] <- LogLike(tau,pi.init,beta.init,sigma.init)

 iter <- iter+1
 k <- k+1
 }
 regmix_em <- list(k = k,
 pi.hat = pi.init,
 sigma.hat = sigma.init,
 beta.hat = beta.init,
 log.lik = loglik[k])
}
###############################################################
##---------Parameters------------------------------------
n <- 400
pi <- c(0.3, 0.4, 0.3)
bet <- matrix(c(1, 1, 1, -1, -1, -1), 2, 3)
sig <- 1
set.seed(1205)
# Generating data
dat <- regmix_sim(n, pi, bet, sig)
dat <- as.matrix(dat)
dat[1:10,] # print generated data row 1 to 10
#ML estimates using EM algorithm
y <- dat[,1]
xmat <- dat[,-1]
pi.init <- pi/pi/length(pi)
sigma.init <- sig/sig
beta.init <- matrix(rnorm(6), 2, 3)
tol <- 1e-5
maxit <- 500

MLE_EM <- regmix_em(y,xmat,pi.init,beta.init,sigma.init,tol,maxit)
#Print Results
MLE_EM$k # number of iterations
MLE_EM$pi.hat 
MLE_EM$beta.hat 
MLE_EM$sigma.hat
MLE_EM$log.lik

#one step EM iteration 
regmix_em1step <- function(y,xmat,pi.init,beta.init,sigma.init){
 tau <- array(NA,dim = c(length(y),length(pi.init)))
 # E step
 for(j in 1:length(pi.init)){
 dem <- SumLike(pi.init,beta.init,sigma.init)
 tau[,j] <- (pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))/dem
 }

 # M step
 for(j in 1:length(pi.init)){
 pi.init[j] <- mysum(tau[,j])/length(y)
 }
 
 for(j in 1:length(pi.init)){
 beta.init[,j] <-
as.matrix(rowSums(t(xmat)%*%(tau[,j]%*%t(y))))/mysum((xmat%*%t(xmat))%*%tau[,j])
}

 sigmaJ <- c()
 for(j in 1:length(pi.init)){
 sigmaJ[j] <- mysum(tau[,j]*(y-xmat%*%beta.init[,j])^2)
 }
 sigma.init <- sqrt(mysum(sigmaJ)/n)

 regmix_em1step <- list(pi.hat = pi.init,
 sigma.hat = sigma.init,
 beta.hat = beta.init)
}
# Implementation
MLE_EM1step <- regmix_em1step(y,xmat,pi.init,beta.init,sigma.init)
# Print Results
MLE_EM1step$pi.hat 
MLE_EM1step$beta.hat
MLE_EM1step$sigma.hat


# Objective function
# negative log-likelihood of finite mixture regression
regmix_em.loglik <- function(pi.init,beta.init,sigma.init){
 compJ <- array(NA,dim = c(length(y),length(pi.init)))
 for(j in 1:3){
 compJ[,j] <- log(pi.init[j]*dnorm(y-xmat%*%beta.init[,j],0,sigma.init))
 }
 loglik <- -mysum(rowSums(compJ))
 return(loglik)
}
# call SQUARM package
library(SQUAREM)
fit <- squarem(par=list(pi.init=MLE_EM$pi.hat, beta.init=MLE_EM$beta.hat, sigma.init=MLE_EM$sigma.hat), objects=regmix_em.loglik, fixptfn=regmix_em1step, pi.init=MLE_EM$pi.hat, beta.init=MLE_EM$beta.hat, sigma.init=MLE_EM$sigma.hat)

```









