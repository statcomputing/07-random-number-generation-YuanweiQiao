---
title: "MaximumLikelihood"
author: "Yuanwei Qiao"
date: "9/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 3.5.1

Consider estimating the location parameter of a Cauchy distribution with a known scale parameter. The density function is
\begin{align*}
  f(x; \theta) = \frac{1}{\pi[1 + (x - \theta)^2]}, 
  \quad x \in R, \quad \theta \in R.
\end{align*}
Let $X_1, \ldots, X_n$ be a random sample of size $n$ and $\ell(\theta)$ the log-likelihood function of $\theta$ based on the sample. 

 
## 1 
Show that
\begin{align*}
  \ell(\theta)
  &= -n\ln \pi - \sum_{i=1}^n \ln [1+(\theta-X_i)^2], \\
  \ell'(\theta)
  &= -2 \sum_{i=1}^n \frac{\theta-X_i}{1+(\theta-X_i)^2}, \\
  \ell''(\theta)
  &= -2 \sum_{i=1}^n \frac{1-(\theta-X_i)^2}{[1+(\theta-X_i)^2]^2},
  \\
  I_n(\theta) &= \frac{4n}{\pi}
  \int_{-\infty}^\infty \frac{x^2dx}{(1+x^2)^3}
  = n/2,
\end{align*}
where $I_n$ is the Fisher information of this sample.
\newline
In this question, we will calculate$\ell(\theta),\ell'(\theta),\ell''(\theta),I_n(\theta)$ as bellow:
\begin{align*}
 \ell(\theta) &=\ln\prod_{i=1}^{n}f(X_i;\theta) \\
 &=\sum_{i=1}^{n} \ln f(X_i;\theta)\\
 &=-n\ln\pi - \sum_{i=1}^n \ln [1+(X_i-\theta)^2]\\
 \ell'(\theta) &=\frac{d}{d\theta}\ell(\theta)\\
 &=-2 \sum_{i=1}^n \frac{\theta-X_i}{1+(\theta-X_i)^2}\\
 \ell''(\theta) &=\frac{d}{d\theta}\ell'(\theta)\\
 &=-2 \sum_{i=1}^n \frac{1-(\theta-X_i)^2}{[1+(\theta-X_i)^2]^2}\\
\end{align*}
The Fisher information of this sample can be defined as:
\begin{align*}
 I_n(\theta) &=\int \ell''(\theta) f(X_i;\theta) dx\\
 &=\frac{2n}{\pi} \int_{-\infty}^{\infty}\frac{1-x^2}{(1+x^2)^2} \frac{1}{1+x^2}dx\\
 &=\frac{2n}{\pi} \int_{-\infty}^{\infty}(\frac{x}{1+x^2})'\frac{1}{1+x^2}dx\\
 &=\frac{2n}{\pi} \int_{-\infty}^{\infty}\frac{1}{1+x^2}d(\frac{x}{1+x^2})\\
 &=\frac{2n}{\pi} \frac{x}{1+x^2}\frac{1}{1+x^2}\bigg|_{-\infty}^{\infty}-\frac{2n}{\pi}\int_{-\infty}^{\infty}\frac{x}{1+x^2}d(\frac{1}{1+x^2})\\
 &=\frac{4n}{\pi} \int_{-\infty}^{\infty}\frac{x^2}{(1+x^2)^3}{dx}\\
\end{align*}
Then we should convert $x$ to $\tan t$, the domain of $x$ is $-\infty$ to $\infty$, so the domain of t is $-\frac{\pi}{2}$ to $\frac{\pi}{2}$:
\begin{align*}
 I_n(\theta) &=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\frac{(\tan t)^2}{[(1+\tan t)^2]^3}{d\tan t}\\
 &=\frac{4n}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}{(\sin t)^2}{(\cos t)^2}dt\\
 &=\frac{n}{2}
\end{align*}
\newpage


## 2 

Set the random seed as 909 and generate a random sample of size $n=10$ with  $\theta=5$. Implement a log likelihood function and plot against $\theta$

```{r code, results='hide'}
set.seed(909)
data <- rcauchy(10,5)             #generate a sample from a Cauchy distribution
loglikelihood <- function(x) {    #calculating the log likelihood
  loglikelihood <- 0
  for (i in 1:length(data)){
    loglikelihood <- loglikelihood-log(pi)-log(1+(x-data[i])^2)
  }
  loglikelihood
}

library("ggplot2")                #plotting log likelihood against theta
ggplot(data.frame(x= c(-20,20)),aes(x = x))+
         stat_function(fun = function(x) loglikelihood(x))+
         labs(x=expression(~theta),y="log likelihood")

```

## 3

Find the MLE of $\theta$ using the Newton–Raphson method with initial values on a grid starting from −10 to 30 with increment 0.5. Summarize the results.

```{r code2, results='hide',warning=FALSE}
library(pracma)
dloglike <- function(x) {         #calculate the derivative of log likelihood function
  d1 <- 0
  for (i in 1:length(data)){
    d1 <- d1-2*(x-data[i])/(1+(x-data[i])^2)
  }
  d1
}
ddloglike <- function(x) {  #calculate the second derivative of log likelihood function
  d2 <- 0
  for (i in 1:length(data)){
    d2 <- d2-2*(1-(x-data[i])^2)/(1+(x-data[i])^2)^2
  }
  d2
}
x_initial <- seq(-10,30,by=0.5)
result <- seq(-10,30,by=0.5)
for (i in 1:length(x_initial)){
  result[i] <- newtonRaphson(dloglike,x_initial[i],dfun = ddloglike)$root 
}
plot(x_initial, result)
```

## 4

Improved the Newton–Raphson method by halving the steps if the likelihood is not improved.
```{r warning=FALSE}
Improved_newtonRaphson<- function (fun, x0, dfun = NULL, myfun = NULL, maxiter = 500, tol = 1e-08, ...) 
{
    stopifnot(is.function(fun))
    if (is.null(dfun)) {
        dfun <- function(x, ...) {
            h <- tol^(2/3)
            (fun(x + h, ...) - fun(x - h, ...))/(2 * h)
        }
    }
    x <- x0
    fx <- fun(x, ...)
    dfx <- dfun(x, ...)
    niter <- 0
    diff <- tol + 1
    while (diff >= tol && niter <= maxiter) {
        niter <- niter + 1
        if (dfx == 0) {
            warning("Slope is zero: no further improvement possible.")
            break
        }
        diff <- -fx/dfx
        if (myfun(x+diff)<=myfun(x)) diff=diff/2 #half the difference if there's no improvement
        x <- x + diff
        diff <- abs(diff)
        fx <- fun(x, ...)
        dfx <- dfun(x, ...)
    }
    if (niter > maxiter) {
        warning("Maximum number of iterations 'maxiter' was reached.")
    }
    return(list(root = x, f.root = fx, niter = niter, estim.prec = diff))
}
for (i in 1:length(x_initial)){
  result[i] <- Improved_newtonRaphson(dloglike, myfun = loglikelihood, x_initial[i],dfun = ddloglike)$root 
}
plot(x_initial, result)
```


## 5

Apply fixed-point iterations using $G(\theta)=\alpha \ell'(\theta) + \theta$, with scaling choices of $\alpha \in \{1, 0.64, 0.25\}$ and the same initial values as above.

```{r code3, results='hide'}
alpha <- c(1, 0.64, 0.25)
fixed_point <- function(dloglike, alpha, x_initial, maxiter = 100, tol = 1e-10){
  x <- x_initial
  for (i in 1:maxiter) {
   x1 <- alpha * dloglike(x) + x
   if (abs(x1 - x) < tol) break
   x <- x1
  }
  if (i == maxiter) 
    print("maximum iteration")
  x
}
result<- array(0, c(3,length(x_initial)))
for (j in 1:length(alpha)){
  for (i in 1:length(x_initial)){
    result[j,i] <- fixed_point(dloglike,alpha=alpha[j],x_initial=x_initial[i])}}
plot(x_initial, result[1,],col="red",xlab = "x_initial", ylab = "result")
points(x_initial, result[2,],col="blue")
points(x_initial, result[3,],col="black")
```

## 6

First use Fisher scoring to find the MLE for $\theta$, then refine the estimate by running Newton-Raphson method. Try the same starting points as above.

```{r code4,results='hide',warning=FALSE}
I=length(data)/2
fisher_score <- function(dloglike, x_initial, maxiter = 100, tol = 1e-10){
  x <- x_initial
  for (i in 1:maxiter) {
   x1 <- x + dloglike(x)/I
   if (abs(x1 - x) < tol) break
   x <- x1
  }
  if (i == maxiter) 
    print("maximum iteration")
  x
}
result <-array(0, c(length(x_initial)))
for (i in 1:length(x_initial)){
  result[i] <- fisher_score(dloglike=dloglike, x_initial = x_initial)}
finalresult <- array(0, c(length(result)))
for (i in 1:length(result)){
  finalresult[i] <- newtonRaphson(dloglike,result[i],dfun = ddloglike) 
}
plot(x_initial, finalresult)
```

## 7

- The Newton-Raphson method doesn't converge if the initial value is far from the true value. 
- Improved the NR method by halving the steps if the likelihood is not improved. This improved the NR method's performance.
- fixed-point method, the performance of this method doesn't depend on the initial value, but it depends on the choice of scaling parameter.

- After choosing the initial value by the fisher scoring method, the Newton-Raphson converges to the true value very fast 



# 3.5.2


## 1

The log-likelihood function with parameter $\theta$ and $x$:
\begin{align*}
 \ell(\theta) = \sum^n_{i=1}{ln(1-\cos{(x-\theta)})} - nln2\pi
\end{align*}
\newline
then plot it between $-\pi$ and $\pi$:
```{r}
data = c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96,
      2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
loglike <- function(x) {           #calculate the log likelihood function
  loglike <- 0
  for (i in 1:length(data)){
    loglike <- loglike+log(1-cos(data[i]-x))-log(2*pi)
  }
  loglike
}

library("ggplot2")                    #plot the log likelihood against theta
ggplot(data.frame(x= c(-pi,pi)),aes(x = x))+
         stat_function(fun = function(x) loglike(x))+
         labs(x=expression(~theta),y="log likelihood")
```

## 2

The expression for $E (X \mid \theta)$ is:
\begin{align*}
E(X|\theta) & =\int^{2\pi}_{0}x\frac{1-\cos(x-\theta)}{2\pi}dx\\
            & =\frac{1}{2\pi}\left[\int^{2\pi}_{0}xdx-\int^{2\pi}_{0}x\cos(x-\theta)dx\right]\\
            & =\frac{1}{2\pi}\left[\frac{1}{2}x^2\big |^{2\pi}_{0}-xsin(x-\theta)\big |^{2\pi}_{0}\right]\\
            & =\frac{1}{2\pi}\left(2\pi^{2}+2\pi\sin\theta\right)\\
            & =\pi+\sin\theta
\end{align*}
\newline \newline
And $\bar X_n$ is 3.236842, so we can get $\theta$:
\begin{align*}
\theta & = \arcsin(mean(x)-\pi)\text{ or }\pi-\arcsin(mean(x)-\pi)\\
       & = 0.095394\text{ or }3.046199
\end{align*}


## 3

Find the MLE for $\theta$ using the Newton--Raphson method initial value $\theta_0 = \tilde\theta_n$. In this question, we will calculate$\ell'(\theta),\ell''(\theta)$ as follow:
\begin{align*}
\ell'(\theta) = \sum^n_{i=1}{\frac{-sin(x-\theta)}{1-cos(x-\theta)}}\\
\ell"(\theta) = \sum^n_{i=1}{\frac{1}{cos(x-\theta)-1}}
\end{align*}
Newton--Raphson method:
```{r}
library(pracma)
dloglike <- function(x) {         
  d1 <- 0
  for (i in 1:length(data)){
    d1 <- d1-sin(data[i]-x)/(1-cos(data[i]-x))
  }
  d1
}
ddloglike <- function(x) {  
  d2 <- 0
  for (i in 1:length(data)){
    d2 <- d2+1/(cos(data[i]-x)-1)
  }
  d2
}
x_initial <- 0.095394
(result1 <- newtonRaphson(dloglike,x_initial,dfun = ddloglike))
x_initial <- 3.046199
(result2 <- newtonRaphson(dloglike,x_initial,dfun = ddloglike))
```

If we start with $\theta = 0.095394$ we get $root = 0.003118157$, and if we start with $\theta = 3.046199$ we get $3.170715$.


## 4

We can just change the initial vaule of $\theta$ to find the root:
```{r}
x_initial <- -2.7
(result3 <- newtonRaphson(dloglike,x_initial,dfun = ddloglike))
x_initial <- 2.7
(result4 <- newtonRaphson(dloglike,x_initial,dfun = ddloglike))
```

If we start with $\theta = -2.7$ we get $-2.668857$ and if we start with $\theta = 2.7$ we get $root = 2.848415$.


## 5

```{r warning=FALSE}
library("pander")
options(digits = 5)
x_initial <- seq(-pi,pi,length = 200)
result <- matrix(0,1,length(x_initial))
for (i in 1:length(x_initial)){
  result[i] <- newtonRaphson(dloglike,x_initial[i],dfun = ddloglike) 
}
plot(x_initial, result, title("Initial values and roots"))
table = rbind(x_initial,result)
set.caption("Initial values and roots")
pander(table,split.table = 100,style = 'rmarkdown')
roots <- c(-3.1125,-2.7866,-2.6689,-2.5094,-2.3883,-2.2979,-2.2322,-1.6627,-1.4475,-0.9544,-0.0031182,-0.81264,2.0072,2.237,2.3747,2.4884,2.8484,3.1707)
freq <- c(11,2,5,6,1,4,1,24,1,19,42,46,8,2,6,2,15,5)
table1 <- rbind(roots,freq) 
print(table1)
```

